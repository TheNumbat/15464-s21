---
layout: post
title: Lecture 8
---

Research follow up: motion graphs/matching/blending

# Thoughts
- Went over historical papers on the subject, mostly review though
- Went over a recent learned motion matching paper, which was pretty impressive for a technique usable for real-time animation (although realistic-ness is less of a critical priority in games). As is common in ML, they train a model that allows them to encode a compressed representation. They look up only relevant animation features and use the model to decompress them into poses rather than doing another lookup into the pose database. The model also allowed them to add extra unknown but salient features as found by the model. Then they go even further and ML all-the-things, adding a network to convert query features to combined query/latent features, and a network to step the features forward instead of looking up into the animation. 
- Their video mentioned preserving features of the animations better than phase-functioned neural network techniques, but I wasn't really sold on that - there's still some award positions/sliding. Their polar bear example seemed more impressive, so I wonder if this technique really relies on a _lot_ of animation data to train on.
- Since they're training an encoder and decoder, shouldn't they basically be using an autoencoder that jointly trains the encode and decode network? It seemed they were doing it separately. Maybe this was just because they started out with separate query and latent feature parameters. 
